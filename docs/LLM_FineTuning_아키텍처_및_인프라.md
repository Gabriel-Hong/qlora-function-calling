# LLM Fine-Tuning 아키텍처 및 인프라

> 상용 LLM + Fine-Tuned LLM 하이브리드 구조, 인프라 선택, GPU 사양/비용을 정리합니다.

---

## 1. 하이브리드 아키텍처: 상용 LLM + Fine-Tuned LLM

상용 LLM이 대화를 주도하고, GEN NX 관련 tool calling만 fine-tuned 모델에 위임하는 구조입니다.

### 1-1. 역할 분리

```
비유로 설명하면:

  상용 LLM (Claude/GPT)  =  총괄 PM
    → 고객(사용자)과 대화하고, 전체 상황을 판단하고, 결과를 설명합니다

  Fine-Tuned LLM         =  구조 전문 엔지니어
    → 기준서를 숙지하고 있고, GEN NX를 어떻게 돌려야 하는지 알고 있습니다

  MCP Server (GEN NX)    =  CAE 프로그램 그 자체
    → 실제로 해석을 돌리는 도구입니다
```

### 1-2. 전체 흐름 (패턴 1: Orchestrator — 추천)

```
  사용자
    │
    │ "10층 RC 건물의 내진설계를 GEN에서 해줘"
    ▼
┌──────────────────────────────────────────────────────┐
│              상용 LLM (Claude / GPT-4)                │
│                                                      │
│  (1) 의도를 분석합니다                                  │
│     "GEN NX 작업이 필요하구나"                          │
│     "이건 내가 직접 하는 것보다 전문가에게 맡기자"         │
│                                                      │
│  (2) 맥락을 정리해서 Fine-Tuned 모델에 넘깁니다          │
│     → 구조물 정보, 설계 조건, 적용 기준 등               │
│                                                      │
│  (5) 결과를 받아서 사용자에게 자연어로 설명합니다          │
│     "내진설계 해석이 완료됐습니다. 결과를 보면..."         │
└──────────────┬───────────────────────────────────────┘
               │
               │ (2) "이 조건으로 GEN NX 돌려줘"
               ▼
┌──────────────────────────────────────────────────────┐
│         Fine-Tuned LLM (Qwen2.5-1.5B QLoRA)          │
│                                                      │
│  이 모델은 두 가지를 학습한 상태입니다:                   │
│  a) 건축구조 도메인 지식 (KBC, ACI, Eurocode 등)        │
│  b) GEN NX tool calling 형식                          │
│                                                      │
│  (3) 도메인 지식을 기반으로 tool call을 생성합니다        │
│                                                      │
│  "10층 RC + 내진등급 I + KBC 2016이면...               │
│   응답스펙트럼 해석을 해야 하고,                         │
│   지반 종류와 지역계수를 설정해야 하고,                   │
│   이 순서로 API를 호출해야겠다"                          │
│                                                      │
│  출력:                                                │
│  [                                                    │
│    {"name": "create_model",                           │
│     "args": {"type":"RC", "stories":10, ...}},        │
│    {"name": "assign_seismic_load",                    │
│     "args": {"code":"KBC2016", "site_class":"S3",...}},│
│    {"name": "run_analysis",                           │
│     "args": {"type":"response_spectrum", ...}}        │
│  ]                                                    │
└──────────────┬───────────────────────────────────────┘
               │
               │ (3) MCP Tool Calls
               ▼
┌──────────────────────────────────────────────────────┐
│                  MCP Server (GEN NX)                  │
│                                                      │
│  (4) 실제 GEN NX 프로그램을 제어합니다                   │
│     → 모델 생성 → 하중 할당 → 해석 실행 → 결과 추출     │
│                                                      │
│  결과를 반환합니다:                                     │
│  {"status":"success",                                 │
│   "max_drift":0.012, "base_shear":1250, ...}          │
└──────────────┬───────────────────────────────────────┘
               │
               │ (4) 결과
               ▼
          상용 LLM이 받아서 사용자에게 자연어로 설명합니다
```

### 1-3. 라우팅 로직

```
사용자 입력
    │
    ▼
┌──────────────────────────┐
│  상용 LLM이 의도를 분류    │
│                          │
│  "GEN NX 작업이 필요한가?" │
└────────┬─────────────────┘
    ┌────┴────┐
    │         │
   No        Yes
    │         │
    ▼         ▼
┌────────┐  ┌──────────────────────────────┐
│ 상용LLM │  │     Fine-Tuned LLM            │
│ 직접응답 │  │                              │
│        │  │  도메인 Q&A → 지식으로 응답     │
│        │  │  도구 필요 → tool call 생성    │
│        │  │  → MCP → GEN NX 실행          │
└────────┘  └──────────────────────────────┘

비유로 설명하면:
  PM이 회의 중에 일반 질문은 본인이 답하고,
  구조 관련 질문이 나오면 "이건 구조 담당자에게 넘기겠습니다"
  하는 것과 같습니다.
```

### 1-4. 실제 구현: 중간 Gateway 서버

이 라우팅을 실행하려면 중간 Gateway 서버가 필요합니다.

```
┌───────────────────────────────────────────────────────────┐
│                    Gateway Server (Python)                 │
│                                                           │
│  사용자 요청이 들어오면:                                     │
│                                                           │
│  1) 상용 LLM에 의도 분류를 요청합니다                        │
│     → "이 요청이 GEN NX 관련인가?"                          │
│                                                           │
│  2-A) 아니면 → 상용 LLM이 직접 응답합니다                    │
│                                                           │
│  2-B) 맞으면 → Fine-tuned 모델에 tool call 생성을 요청합니다 │
│       → MCP 서버로 실행합니다                               │
│       → 결과를 상용 LLM에 넘겨서 자연어 응답을 생성합니다     │
│                                                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐    │
│  │ Claude API    │  │ Local LLM    │  │ MCP Client   │    │
│  │ (상용)        │  │ (Fine-tuned) │  │ (GEN NX)     │    │
│  │              │  │ localhost    │  │              │    │
│  │              │  │  :8000       │  │              │    │
│  └──────────────┘  └──────────────┘  └──────────────┘    │
└───────────────────────────────────────────────────────────┘
```

### 1-5. 대안 패턴: MCP Proxy

Fine-tuned 모델을 MCP 서버 안에 내장시키는 방법도 있습니다.

```
  상용 LLM (Claude)
    │
    │  Claude 입장에서는 그냥 MCP tool을 호출하는 것입니다:
    │  analyze_structure("10층 RC 건물 내진설계")
    ▼
┌──────────────────────────────────────────┐
│       MCP Server (Smart Proxy)            │
│                                          │
│  안에 Fine-tuned LLM이 내장되어 있습니다    │
│  → Fine-tuned 모델이 알아서 판단합니다      │
│  → GEN NX API를 어떤 순서로 호출할지 결정   │
│  → 실행 후 결과를 반환합니다                │
│                                          │
│  Claude는 내부 구조를 모릅니다              │
│  그냥 "결과"만 받아서 사용자에게 설명합니다   │
└──────────────────────────────────────────┘

비유로 설명하면:
  PM이 "구조 해석 좀 해줘"라고 말만 하면
  구조팀이 알아서 프로그램을 돌리고 결과만 가져다주는 것입니다.
  PM은 GEN NX를 어떻게 돌리는지 알 필요가 없습니다.
```

### 1-6. 패턴 비교

```
┌──────────────┬──────────────────────┬──────────────────────┐
│              │ 패턴 1: Orchestrator  │ 패턴 2: MCP Proxy     │
├──────────────┼──────────────────────┼──────────────────────┤
│ 구조         │ Gateway가 라우팅      │ MCP 서버 안에 LLM 내장 │
├──────────────┼──────────────────────┼──────────────────────┤
│ 상용 LLM은   │ "전문가에게 넘겨야지"  │ "그냥 tool 호출하자"   │
│              │ 라는 판단을 직접 합니다│ 내부 구조를 모릅니다   │
├──────────────┼──────────────────────┼──────────────────────┤
│ 장점         │ 세밀한 제어가 가능     │ 구현이 단순합니다      │
│              │ 중간 과정이 투명       │ 기존 MCP 생태계 활용   │
├──────────────┼──────────────────────┼──────────────────────┤
│ 단점         │ Gateway 구현이 필요   │ MCP 서버가 무거워집니다│
├──────────────┼──────────────────────┼──────────────────────┤
│ 추천 상황     │ 복잡한 다단계 작업    │ 단순한 도구 호출       │
└──────────────┴──────────────────────┴──────────────────────┘

추천: 패턴 1 (Orchestrator)
→ GEN NX 작업은 다단계 순차 호출이 많으므로 세밀한 제어가 필요합니다
```

### 1-7. Fine-Tuning 데이터셋 구성

이 하이브리드 구조에서 fine-tuned 모델이 학습해야 할 데이터는 두 종류입니다.

```
① GEN NX Tool Calling 데이터 (60%)
  User: "10층 RC 건물 응답스펙트럼 해석해줘"
  Assistant: [
    {"name":"create_model", "args":{...}},
    {"name":"assign_seismic_load", "args":{...}},
    {"name":"run_analysis", "args":{...}}
  ]

② 도메인 지식 Q&A 데이터 (40%)
  User: "KBC 2016과 Eurocode 8의 내진설계 차이점은?"
  Assistant: "가장 큰 차이는 지진하중 산정 방식에 있습니다. ..."

이 두 가지를 함께 학습시키면:
→ GEN NX tool calling도 잘 하고
→ 왜 그렇게 호출해야 하는지 도메인 판단도 할 수 있는 모델이 됩니다.
```

---

## 2. 인프라: 쿠버네티스가 아니라 Docker Compose

이 규모에서는 쿠버네티스가 과합니다.

```
쿠버네티스가 해결하는 문제:
  → 수십~수백 대의 서버에서 컨테이너를 자동 배포/관리/스케일링

지금 상황:
  → 노트북 1대, GPU 1개, 서비스 3~4개

비유로 설명하면:
  3명이 앉을 테이블이 필요한데 웨딩홀을 빌리는 것과 같습니다.
```

### 2-1. 규모별 추천 구성

```
┌───────────────────┬────────────────────┬───────────────────────┐
│ 규모               │ 추천 기술           │ 이유                   │
├───────────────────┼────────────────────┼───────────────────────┤
│ ★ 지금 (개발/PoC)  │ Docker Compose     │ 충분하고 간단합니다      │
│                   │ 또는 그냥 프로세스    │                       │
├───────────────────┼────────────────────┼───────────────────────┤
│ 팀 내부 서비스      │ Docker Compose     │ 서버 1~2대면 충분합니다  │
│ (사용자 5~20명)    │ + Nginx            │                       │
├───────────────────┼────────────────────┼───────────────────────┤
│ 사내 서비스        │ Docker Compose     │ 오토스케일링 필요 없으면  │
│ (사용자 100명+)    │ 또는 K8s 검토 시작   │ 아직 Compose로 가능     │
├───────────────────┼────────────────────┼───────────────────────┤
│ 외부 서비스        │ Kubernetes         │ 이때부터 의미 있습니다    │
│ (불특정 다수,      │ + GPU Operator     │ 오토스케일링, 무중단배포  │
│  고가용성 필요)    │                    │ 장애복구 등이 필요       │
└───────────────────┴────────────────────┴───────────────────────┘
```

### 2-2. Docker Compose 구성

```
┌─────────────────────────────────────────────────────────┐
│                   Docker Compose 구성                    │
│                                                         │
│  ┌─────────────────────────────────────────────────┐    │
│  │  gateway (Python FastAPI)          port: 8080    │    │
│  │  → 사용자 요청을 받고 라우팅합니다                 │    │
│  └─────────────────────────────────────────────────┘    │
│                                                         │
│  ┌─────────────────────────────────────────────────┐    │
│  │  local-llm (vLLM 또는 Ollama)      port: 8000    │    │
│  │  → Fine-tuned 모델 서빙, GPU 사용                │    │
│  └─────────────────────────────────────────────────┘    │
│                                                         │
│  ┌─────────────────────────────────────────────────┐    │
│  │  mcp-server (GEN NX 연동)          port: 8001    │    │
│  │  → MCP 프로토콜로 GEN NX 제어                     │    │
│  └─────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────┘
```

### 2-3. 단계별 로드맵

```
지금 (개발)           완성 후 (패키징)        확장 시 (프로덕션)

터미널 3개로          Docker Compose로       Kubernetes로
각각 실행             한 번에 올리기          오토스케일링

$ python gateway.py  docker compose up      kubectl apply -f
$ ollama serve       ← 한 줄               ← 필요해지면
$ python mcp.py                               그때 전환
    │                      │                      │
    ▼                      ▼                      ▼
  빠른 개발              배포 가능한 형태         대규모 서비스
  빠른 디버깅            팀원도 실행 가능         고가용성
```

---

## 3. 로컬 GPU 사양 & 비용

### 3-1. RTX 3060 Laptop (6GB) 가능 범위

```
┌────────────────────────┬──────────┬──────────┬───────────┐
│ 모델                    │ VRAM 사용 │ 학습 가능 │ 서빙 가능  │
├────────────────────────┼──────────┼──────────┼───────────┤
│ Qwen2.5-0.5B (4-bit)  │ ~0.8GB   │ ✅ 여유   │ ✅ 여유   │
│ Qwen2.5-1.5B (4-bit)  │ ~1.5GB   │ ✅ 가능   │ ✅ 여유   │
│ Qwen2.5-3B (4-bit)    │ ~2.5GB   │ ⚠️ 빡빡  │ ✅ 가능   │
│ Qwen2.5-7B (4-bit)    │ ~5GB     │ ❌ 불가   │ ⚠️ 빡빡  │
│ Qwen2.5-14B (4-bit)   │ ~9GB     │ ❌ 불가   │ ❌ 불가   │
└────────────────────────┴──────────┴──────────┴───────────┘

"학습 가능"과 "추론 가능"이 다른 이유:
  추론: 모델 가중치만 VRAM에 올리면 됩니다 → 1.5B 4-bit = ~1.5GB
  학습: 모델 가중치 + 옵티마이저 + 그래디언트 + 임시 버퍼 → 1.5B QLoRA = ~3.5~4GB

추천: Qwen2.5-1.5B로 학습하고, 1.5B로 서빙하는 것이 가장 안정적입니다
```

### 3-2. 1.5B 모델로 충분한가?

```
비유로 설명하면:

  GPT-4:      ~1조 8천억 파라미터 = 백과사전 수만 권을 읽은 박사
  Qwen-1.5B:  15억 파라미터        = 전공 교과서 몇 권을 읽은 학부생

  학부생이 박사만큼 모든 걸 알 수는 없지만,
  "건축구조 기준서에 따른 tool calling"이라는 좁은 범위에서는
  집중 훈련을 받으면 충분히 잘 할 수 있습니다.

실제 사례:
  Salesforce xLAM-1B (10억 파라미터):
  → Function Calling 전용으로 학습시킨 모델
  → BFCL 벤치마크에서 GPT-4-turbo와 비슷한 수준
  → "하나를 잘하는 모델"은 1B~1.5B로도 충분히 만들 수 있습니다
```

### 3-3. 서빙 중 다른 작업 가능 여부

```
1.5B 4-bit 모델 서빙 시 VRAM ~1.5~2GB 사용 → 약 4GB 남음

┌────────────────────────┬──────────┬───────────────────────┐
│ 작업                    │ VRAM 필요 │ LLM 서빙과 동시 가능?  │
├────────────────────────┼──────────┼───────────────────────┤
│ 웹 브라우징, 문서 작업   │ ~0.3GB   │ ✅ 전혀 문제 없습니다  │
│ VS Code, 일반 개발      │ ~0.2GB   │ ✅ 전혀 문제 없습니다  │
│ 유튜브/영상 재생         │ ~0.5GB   │ ✅ 괜찮습니다          │
│ 게임 (가벼운)           │ ~2GB     │ ⚠️ 가능하지만 버벅임   │
│ 게임 (AAA급)           │ ~4GB+    │ ❌ VRAM 부족           │
│ 다른 AI 모델 동시 실행   │ ~2GB+    │ ⚠️ 모델 크기에 따라    │
└────────────────────────┴──────────┴───────────────────────┘
```

### 3-4. 클라우드 GPU vs 로컬 비용 비교

```
┌──────────────────────┬────────────┬──────────────────────────┐
│ 서비스                │ GPU        │ 월 비용 (24시간 상시 기준)  │
├──────────────────────┼────────────┼──────────────────────────┤
│ AWS (g5.xlarge)      │ A10G 24GB  │ ~$550/월                 │
│ GCP (g2-standard-4)  │ L4 24GB    │ ~$450/월                 │
│ Azure (NC4as T4)     │ T4 16GB    │ ~$380/월                 │
├──────────────────────┼────────────┼──────────────────────────┤
│ RunPod               │ RTX 4090   │ ~$300/월                 │
│ Vast.ai              │ RTX 3090   │ ~$150~200/월             │
│ Lambda Labs          │ A10 24GB   │ ~$350/월                 │
└──────────────────────┴────────────┴──────────────────────────┘

1.5B 모델 서빙 용도라면 로컬 GPU로 충분하므로 비용이 들지 않습니다.

비유로 설명하면:
  클라우드 GPU = 렌터카 (매달 요금 나감)
  로컬 GPU     = 내 차 (이미 있으니 기름값만)
  1.5B 모델 서빙 = 시내 출퇴근 수준 → 내 차로 충분합니다
```

---

## 4. 데이터 양 & 학습 시간 수치

### 4-1. 건축구조 + GEN NX 데이터셋 구성 (총 2,500개 목표)

```
┌────────────────────────────────────┬──────┬──────────────────┐
│ 카테고리                            │ 비율  │ 개수              │
├────────────────────────────────────┼──────┼──────────────────┤
│ (1) GEN NX 단일 Tool Calling       │ 35%  │ ~875개           │
│ (2) GEN NX 다단계 Tool Calling     │ 20%  │ ~500개           │
│ (3) 건축구조 도메인 Q&A             │ 25%  │ ~625개           │
│ (4) 기준서 비교/판단 질문            │ 10%  │ ~250개           │
│ (5) 부정 예시 (관련 없는 질문)       │ 10%  │ ~250개           │
├────────────────────────────────────┼──────┼──────────────────┤
│ 합계                               │ 100% │ 2,500개          │
└────────────────────────────────────┴──────┴──────────────────┘
```

### 4-2. 데이터 양에 따른 성능 변화

```
┌───────────┬──────────────┬──────────────┬──────────────┬─────────────┐
│ 데이터 수  │ Tool Name    │ Param        │ JSON         │ 체감 수준    │
│           │ Accuracy     │ Accuracy     │ Validity     │             │
├───────────┼──────────────┼──────────────┼──────────────┼─────────────┤
│ 100개     │ ~40%         │ ~25%         │ ~60%         │ 못 쓰는 수준 │
│ 300개     │ ~60%         │ ~45%         │ ~75%         │ 가끔 맞음   │
│ 500개     │ ~75%         │ ~60%         │ ~85%         │ 반은 맞음   │
│ 1,000개   │ ~85%         │ ~75%         │ ~93%         │ 쓸만해짐    │
│ 2,000개   │ ~93%         │ ~85%         │ ~98%         │ 꽤 좋음 ★   │
│ 3,000개   │ ~96%         │ ~90%         │ ~99%         │ 실용적 ★★   │
│ 5,000개   │ ~97%         │ ~92%         │ ~100%        │ 거의 최대치  │
└───────────┴──────────────┴──────────────┴──────────────┴─────────────┘

추천: 2,000~3,000개 구간이 노력 대비 성능이 가장 좋습니다.
```

### 4-3. 데이터 제작 방법 (추천: LLM 생성 + 수동 검수, 약 5~7일)

```
Step 1: 시드 데이터 수동 작성 (100~200개, 2~3일)
  → 각 카테고리별로 20~40개씩 직접 작성합니다
  → 이게 품질의 기준이 됩니다

Step 2: Claude/GPT로 확장 (2,000개, 1~2일)
  → 시드 데이터를 보여주고 변형된 예시를 대량 생성합니다
  → tool 파라미터 값, 질문 표현, 건물 유형 등을 다양하게 변경합니다

Step 3: 검수 (전체 2,500개, 1~2일)
  → JSON 유효성 자동 검사 (스크립트)
  → tool name이 실제 존재하는지 확인
  → 무작위로 200~300개 샘플링하여 수동 검증
```

### 4-4. 학습 소요 시간

```
RTX 3060 Laptop + Qwen2.5-1.5B + QLoRA 기준:

┌──────────────┬──────────┬────────────────────────────────┐
│ 데이터 수     │ 학습 시간 │ 설정                            │
├──────────────┼──────────┼────────────────────────────────┤
│ 500개 × 3ep  │ ~15분    │ batch=1, grad_accum=8, r=8     │
│ 1,000개 × 3ep│ ~30분    │                                │
│ 2,000개 × 3ep│ ~1시간   │                                │
│ 2,500개 × 3ep│ ~1.5시간 │                                │
│ 3,000개 × 3ep│ ~2시간   │                                │
└──────────────┴──────────┴────────────────────────────────┘

학습 자체는 오래 안 걸립니다.
시간이 많이 드는 건 데이터를 만드는 과정입니다.
```

### 4-5. 전체 프로젝트 수치

```
┌────────────────────┬────────────────────────────────────────────┐
│ 모델               │ Qwen2.5-1.5B-Instruct (4-bit 양자화)       │
│ 학습 방식           │ QLoRA (r=8 또는 16)                        │
│ 학습 시 VRAM        │ ~3.5~4GB / 6GB                            │
│ 서빙 시 VRAM        │ ~1.5~2GB / 6GB                            │
├────────────────────┼────────────────────────────────────────────┤
│ 목표 데이터 수      │ 2,000~3,000개                              │
│ 데이터 제작 기간    │ 5~7일 (LLM 보조 + 수동 검수)               │
│ 학습 시간           │ 1~2시간                                    │
├────────────────────┼────────────────────────────────────────────┤
│ 예상 성능 (학습 후) │ Tool Name Acc: 93~96%                      │
│                    │ Param Acc: 85~92%                          │
│                    │ JSON Valid: 98~100%                        │
├────────────────────┼────────────────────────────────────────────┤
│ 추론 속도           │ ~30 tokens/sec (응답 1개당 0.5~2초)         │
│ 서빙 중 다른 작업   │ 가능 (VRAM 4GB 여유)                       │
│ GPU 서버 비용       │ $0 (로컬 GPU 사용)                         │
├────────────────────┼────────────────────────────────────────────┤
│ 전체 프로젝트 기간  │ 환경 구축 2일 + 데이터 7일 + 학습/평가 5일   │
│                    │ = 약 2주                                   │
└────────────────────┴────────────────────────────────────────────┘
```
