# LLM Fine-Tuning 핵심 개념

> Fine-Tuning을 시작하기 전에 알아야 할 기본 개념들을 정리합니다.

---

## 1. CUDA

GPU로 계산을 돌리게 해주는 NVIDIA의 프로그래밍 플랫폼입니다.

```
CPU: 복잡한 작업 1개를 빠르게      →  코어 8~16개
GPU: 단순한 작업 수천개를 동시에    →  코어 3,000~10,000개

LLM은 결국 거대한 행렬 곱셈의 연속이고,
이걸 GPU에서 돌리려면 CUDA가 필요합니다.

PyTorch 코드 → CUDA → GPU 드라이버 → RTX 3060 GPU
              ↑
         이 다리 역할을 하는 게 CUDA
```

CUDA 버전이 중요한 이유: PyTorch가 특정 CUDA 버전에 맞춰 빌드되기 때문에 버전이 안 맞으면 GPU를 아예 못 씁니다.

---

## 2. HuggingFace 모델 파일 구조

모델을 다운로드하면 숫자로 이루어진 가중치(weight) 파일 + 설정 파일의 묶음입니다.

```
Qwen2.5-1.5B-Instruct/
├── config.json                 ← 모델 구조 정의 (레이어 수, 차원 등)
├── tokenizer.json              ← 토크나이저 (텍스트 ↔ 숫자 변환 사전)
├── model.safetensors           ← ★ 핵심: 가중치 파일 (수십억 개의 float 숫자)
├── generation_config.json      ← 생성 설정 (temperature, top_p 등)
└── special_tokens_map.json
```

`model.safetensors` 안에 들어있는 것:

```
실체: 다차원 텐서(행렬)들의 모음

예시 (극도로 단순화):
{
  "layers.0.attention.q_proj.weight": [[0.0012, -0.0345, ...], ...],  ← (4096 x 4096) 행렬
  "layers.0.attention.k_proj.weight": [[0.0078, 0.0156, ...], ...],
  "layers.0.mlp.gate_proj.weight":   [[0.0234, -0.0089, ...], ...],
  ...
  // 1.5B 모델 = 이런 행렬이 수백 개, 총 15억 개의 숫자
}
```

임베딩 벡터도 이 안에 포함되어 있습니다:

```
"embed_tokens.weight": [[...], [...], ...]   ← (어휘 크기 x 차원) 행렬
                                                예: (151,936 x 1,536)

각 행 = 하나의 토큰에 대한 임베딩 벡터
"안녕" → 토큰 ID 12345 → 12345번째 행 벡터 → [0.023, -0.015, 0.089, ...]
```

정리하면:

```
모델 파일 = 수십억 개의 float 숫자(가중치)를 효율적으로 저장한 바이너리
         = 학습을 통해 최적화된 행렬들의 집합
         = 이 숫자들이 "지식"이자 "능력"
```

---

## 3. LoRA (Low-Rank Adaptation)

원본 가중치는 얼리고, 작은 행렬 2개만 추가해서 학습하는 방법입니다.

### 문제 상황

```
Qwen2.5-1.5B 모델 = 15억 개의 파라미터 (가중치 숫자)

전체 파라미터를 다 학습하려면 (Full Fine-Tuning):
→ 모델 가중치:      ~3GB
→ 옵티마이저 상태:   ~6GB  (Adam은 파라미터당 2개 추가 저장)
→ 그래디언트:        ~3GB
→ 합계:             ~12GB 이상 → 6GB GPU로 불가능
```

### LoRA의 동작 원리

```
[원래 방식: Full Fine-Tuning]

입력 → ┌──────────────┐ → 출력
       │ W (4096x4096) │         ← 이 거대한 행렬 전체를 업데이트
       │ 1,677만 개     │            = 메모리 폭발
       └──────────────┘


[LoRA 방식]

       ┌──────────────┐
       │ W (4096x4096) │  ← freeze (얼림, 학습 안 함)
       └──────┬───────┘
              │
입력 → ───────┤
              │   ┌─────────┐   ┌─────────┐
              └──→│ A (4096x8)│ → │ B (8x4096)│ → 더함
                  │  32,768개 │   │  32,768개 │
                  └─────────┘   └─────────┘
                       ↑              ↑
                  이 작은 행렬 2개만 학습 = 65,536개
                  (원래 1,677만 대비 ~0.4%)
```

여기서 `8`이 rank (r) 값입니다. rank가 클수록 표현력은 좋지만 메모리를 더 먹습니다.

```
r=8:   학습 파라미터 ~0.4%   → 메모리 적음, 대부분 충분
r=16:  학습 파라미터 ~0.8%   → 조금 더 좋은 품질
r=32:  학습 파라미터 ~1.6%   → 복잡한 태스크용
```

---

## 4. QLoRA = 양자화(Quantization) + LoRA

```
LoRA:   원본 모델을 FP16 (16-bit)으로 로딩 + 작은 행렬 학습
        → 1.5B 모델 기준 ~3GB + 학습 오버헤드 = 여전히 빡빡

QLoRA:  원본 모델을 4-bit로 압축 로딩 + 작은 행렬 학습
        → 1.5B 모델 기준 ~1GB + 학습 오버헤드 = 6GB에서 가능!
```

```
[메모리 비교]

Full FT:  ████████████████████████  ~12GB+   ← 6GB GPU 불가
LoRA:     ██████████████            ~6-7GB   ← 6GB GPU 아슬아슬
QLoRA:    ████████                  ~3-4GB   ← 6GB GPU OK ✓
```

### QLoRA의 품질 손실

놀라울 정도로 적습니다. 대부분의 태스크에서 Full Fine-Tuning 대비 1% 이내의 차이입니다.

```
NF4(NormalFloat4)가 손실을 최소화하는 원리:

  LLM 가중치는 0 근처에 집중된 정규분포를 따릅니다.
  NF4는 이 분포에 최적화되어 있어서:
  → 0 근처 (빈도 높은 곳): 촘촘하게 나눕니다 (정밀도 높음)
  → 양 끝 (빈도 낮은 곳): 듬성듬성 나눕니다 (어차피 드묾)
  → 16-bit → 4-bit으로 줄여도 실질적 정보 손실이 극히 작습니다
```

모델 크기별 양자화 손실:

```
65B 모델의 4-bit:   거의 손실 없음 (파라미터가 워낙 많아 여유 있음)
7B 모델의 4-bit:    미미한 손실
1.5B 모델의 4-bit:  약간의 손실 가능 (~1~3%)
0.5B 모델의 4-bit:  체감 가능한 손실 (~3~5%)
```

프로젝트 기준 (Qwen2.5-1.5B + QLoRA):

```
                        Full FT     QLoRA      차이
Tool Name Accuracy:      97%    →    95~96%     1~2%
Parameter Accuracy:      93%    →    90~92%     1~3%
JSON Validity:          100%    →    99~100%    0~1%

트레이드오프:
성능 손실  1~3%
VRAM 절감  ~70% (12GB → 3~4GB)

→ 1~3%의 성능을 포기하고 70%의 메모리를 절약하는 거래입니다.
→ 6GB GPU에서는 QLoRA 없이 학습 자체가 불가능하므로 선택의 여지가 없습니다.
```

---

## 5. 데이터셋: 형식과 만드는 방법

### PDF를 그냥 넣는 것이 아닙니다

SFT(Supervised Fine-Tuning)에서는 "이렇게 물어보면 → 이렇게 대답해라" 쌍을 만듭니다.

```jsonl
{"messages": [
  {"role": "system", "content": "너는 건축구조 전문가야."},
  {"role": "user", "content": "KBC 2016에서 내진설계 적용 대상은?"},
  {"role": "assistant", "content": "KBC 2016 기준으로 ...에 해당하는 건축물은 내진설계를 적용해야 합니다."}
]}
```

이걸 수천 개 만드는 것입니다. 한 줄이 하나의 학습 예시입니다.

### PDF → 데이터셋 변환 과정

```
[원본 자료]           [가공 과정]              [학습 데이터]

PDF, 매뉴얼,    →   Q&A 쌍으로 변환     →    JSONL 파일
논문, 기준서          (수동 or LLM 활용)        (messages 형식)

예시:
┌──────────────┐     ┌───────────────────┐     ┌──────────────────┐
│ KBC 2016.pdf │ →   │ Q: 내진설계 기준은? │ →   │ {"messages": [   │
│              │     │ A: KBC 2016에서는  │     │   {"role":"user", │
│ "6층 이상 또는│     │    6층 이상 또는    │     │    "content":..} │
│  연면적 ...  │     │    연면적 ...      │     │   {"role":"asst", │
│  내진설계를   │     │                   │     │    "content":..} │
│  적용한다"   │     │                   │     │ ]}               │
└──────────────┘     └───────────────────┘     └──────────────────┘
```

### Fine-Tuning vs RAG 차이

```
RAG:          PDF 원문을 검색해서 참고하며 답변 (모델 자체는 변하지 않음)
Fine-Tuning:  Q&A 쌍으로 모델 가중치 자체를 업데이트 (모델이 "학습"함)
```

### 목표에 따라 데이터셋 구성이 달라집니다

```
┌─────────────────────────────────────────────────────────────────┐
│ Tool Calling 학습                                               │
│ → 목표: 함수를 정확한 JSON 형식으로 호출하게 만들기                │
│ → 데이터 예시:                                                  │
│   User: "서울 날씨 알려줘"                                      │
│   Assistant: {"name":"get_weather","arguments":{"city":"Seoul"}} │
│ → 학습하는 것: 출력 형식(JSON), 도구 선택 능력, 파라미터 매핑      │
├─────────────────────────────────────────────────────────────────┤
│ 도메인 지식 학습                                                 │
│ → 목표: 건축구조 기준서 내용을 정확히 알고 답변하게 만들기          │
│ → 데이터 예시:                                                  │
│   User: "ACI 318-19에서 콘크리트 최소 피복 두께는?"               │
│   Assistant: "ACI 318-19 Section 20.6.1에 따르면, ..."          │
│ → 학습하는 것: 도메인 지식, 기준서 해석 능력, 전문 용어            │
└─────────────────────────────────────────────────────────────────┘

핵심: 데이터셋의 구성이 곧 모델의 능력을 결정합니다.
Tool calling 데이터를 넣으면 tool calling을 잘 하고,
도메인 지식 Q&A를 넣으면 도메인 전문가가 됩니다.
```

---

## 6. Fine-Tuning 성능 향상 수치

### 태스크 유형별 향상 폭

```
┌──────────────────────┬──────────────┬──────────────┬────────────┐
│ 태스크 유형           │ Base 모델     │ Fine-Tuned   │ 향상 폭    │
├──────────────────────┼──────────────┼──────────────┼────────────┤
│ Function Calling     │ 10~30%       │ 90~97%       │ +60~80%p   │
│ 도메인 특화 Q&A       │ 30~50%       │ 70~90%       │ +30~50%p   │
│ 분류/감정분석         │ 50~65%       │ 85~95%       │ +20~35%p   │
│ 일반 대화 품질        │ 60~70%       │ 75~85%       │ +10~20%p   │
│ 범용 지식/추론        │ 70~80%       │ 72~83%       │ +2~5%p     │
└──────────────────────┴──────────────┴──────────────┴────────────┘
```

```
향상 폭의 법칙:

  Base 모델이 못하는 것을 가르칠 때     → 향상 극적
  Base 모델이 애매하게 하는 것을 교정    → 향상 중간
  Base 모델이 이미 잘하는 것을 강화      → 향상 미미
```

### 왜 이런 차이가 나는가

```
[Function Calling — 향상이 극적인 이유]

Base 모델에게 "서울 날씨 알려줘"라고 하면:
  Base:  "서울의 날씨는 사계절이 뚜렷하며..."  ← 그냥 자연어
  FT:    {"name":"get_weather","arguments":{"city":"Seoul"}} ← 정확한 JSON

Base 모델은 "JSON으로 함수를 호출해야 한다"는 것 자체를 모릅니다.
= 모르는 걸 가르친 것 → 0에서 90으로 올라갑니다

[범용 지식 — 향상이 미미한 이유]

"프랑스의 수도는?" 같은 건 이미 Base 모델이 알고 있습니다.
= 이미 아는 걸 반복한 것 → 80에서 83으로 올라갑니다
```

### Catastrophic Forgetting (파국적 망각)

특정 능력이 올라가면 다른 능력이 내려갈 수 있습니다.

```
                      Base       Fine-Tuned
Function Calling:      15%   →    95%   ↑↑↑  목표 태스크 향상
일반 한국어 대화:       80%   →    72%   ↓    약간 퇴화
수학 추론:             65%   →    58%   ↓    약간 퇴화
코드 생성:             70%   →    64%   ↓    약간 퇴화
```

```
비유로 설명하면:

  수학을 잘하던 학생에게 6개월간 영어만 집중 공부시키면:
  → 영어: 크게 향상
  → 수학: 안 쓰니까 감이 떨어짐

완화 방법:
  1. 데이터에 범용 대화 10~20% 섞기
  2. LoRA rank를 너무 크게 잡지 않기 (r=8~16)
  3. 학습을 너무 오래 하지 않기 (epoch 3~5 이내)
```

---

## 7. GPU 서빙: 매 질문마다 GPU 연산이 필요합니다

### 모델은 "답변집"이 아니라 "뇌"입니다

```
비유로 설명하면:

  ❌ 사전 (Dictionary)
     "서울 날씨" → 미리 저장된 답변을 찾아서 반환
     → 이건 DB 검색입니다

  ✅ 사람의 뇌 (Brain)
     "서울 날씨" → 질문을 이해하고, 생각하고, 답을 만들어냄
     → LLM이 하는 방식입니다
     → 이 "생각하는 과정"이 GPU 연산입니다
```

### 실제로 GPU에서 일어나는 일

```
사용자: "서울 날씨 알려줘"

  ① 토큰화
     "서울 날씨 알려줘" → [15234, 8891, 23456, 7890]

  ② Forward Pass (순전파) — 핵심 연산
     토큰들이 모델의 레이어를 하나씩 통과합니다

     입력 토큰
       │
       ▼
     [Layer 1]  행렬 곱셈 → Attention → 행렬 곱셈
       ▼
     [Layer 2]  행렬 곱셈 → Attention → 행렬 곱셈
       ▼
      ...       (Qwen2.5-1.5B는 이런 레이어가 28개)
       ▼
     [Layer 28] 행렬 곱셈 → Attention → 행렬 곱셈
       ▼
     다음 토큰 확률 계산
     → "get" (72%), "서울" (15%), "날씨" (8%), ...
     → "get" 선택

  ③ 한 글자씩 반복 (Auto-regressive)
     위 과정을 출력 토큰마다 반복합니다

     get → _weather → ( → "city" → : → "Seoul" → ) → ...
     ↑      ↑          ↑
     매번   매번       매번 GPU가 28개 레이어 전체를 계산

  토큰 30개를 생성하려면 → 이 계산을 30번 반복합니다
```

### 모델 로딩 vs 추론

```
┌────────────────────┬──────────┬────────────────────────────┐
│                    │ 횟수      │ 하는 일                     │
├────────────────────┼──────────┼────────────────────────────┤
│ Fine-Tuning (학습)  │ 1번      │ 모델 가중치를 업데이트합니다  │
│                    │          │ → 완료 후 파일로 저장        │
├────────────────────┼──────────┼────────────────────────────┤
│ 모델 로딩          │ 서버 시작 │ 파일 → GPU VRAM으로 올립니다 │
│                    │ 시 1번   │ → 5~15초 소요              │
├────────────────────┼──────────┼────────────────────────────┤
│ 추론 (Inference)   │ 매번     │ 질문마다 GPU로 계산합니다    │
│                    │          │ → 0.5~2초 소요             │
└────────────────────┴──────────┴────────────────────────────┘

모델을 매번 새로 로딩할 필요는 없고,
한 번 VRAM에 올려놓으면 계속 사용할 수 있습니다.
```

### GPU 사용 패턴

```
GPU는 질문이 들어올 때만 바쁩니다:

시간 ──────────────────────────────────────────→

GPU   ▁▁▁▁▁▁█████▁▁▁▁▁▁▁▁▁▁███▁▁▁▁▁▁▁▁████▁▁▁▁▁
사용률      ↑               ↑              ↑
         질문1            질문2          질문3
        (1초간 바쁨)     (0.7초)       (0.8초)

→ 질문이 안 들어오는 동안에는 다른 작업이 자유롭습니다
→ 모델이 VRAM에 "올라가 있기만 한" 상태입니다

GPU로도 돌릴 수 있지만 속도 차이가 큽니다:
  RTX 3060 (GPU):  ~30 tokens/s   → 거의 실시간 대화
  CPU (i7 급):     ~3 tokens/s    → 한 글자씩 느리게
```
