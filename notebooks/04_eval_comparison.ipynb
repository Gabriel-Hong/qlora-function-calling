{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluation: Base vs Fine-Tuned Model Comparison\n",
    "\n",
    "This notebook performs a full evaluation comparison between the base model and the fine-tuned model:\n",
    "- Load test data and tool schemas\n",
    "- Run inference with both base and fine-tuned models\n",
    "- Compute evaluation metrics (tool name accuracy, parameter accuracy, JSON validity, hallucination rate)\n",
    "- Visualize results with comparison charts\n",
    "- Perform sample-by-sample error analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from src.data_utils import load_jsonl, get_tokenizer\n",
    "from src.eval_metrics import (\n",
    "    parse_tool_calls_from_output, compute_full_evaluation,\n",
    "    measure_latency, measure_vram_usage,\n",
    ")\n",
    "sns.set_theme(style=\"whitegrid\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_PATH = \"../models/checkpoints/final_adapter\"  # Update after training\n",
    "TEST_DATA = \"../data/processed/test.jsonl\"\n",
    "TOOLS_SCHEMA = \"../data/samples/gennx_tool_schemas_tier1.json\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data and Tools"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_samples = load_jsonl(TEST_DATA)\n",
    "tools = json.load(open(TOOLS_SCHEMA))\n",
    "available_tools = [t[\"function\"][\"name\"] for t in tools]\n",
    "tokenizer = get_tokenizer(BASE_MODEL)\n",
    "print(f\"Test samples: {len(test_samples)}\")\n",
    "print(f\"Available tools: {len(available_tools)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Run Inference on All Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_inference(model, tokenizer, test_samples, tools):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for sample in test_samples:\n",
    "        # Get messages up to user turn\n",
    "        msgs = []\n",
    "        for m in sample[\"messages\"]:\n",
    "            msgs.append(m)\n",
    "            if m[\"role\"] == \"user\":\n",
    "                break\n",
    "        # Get reference tool calls\n",
    "        for m in sample[\"messages\"]:\n",
    "            if m[\"role\"] == \"assistant\" and \"tool_calls\" in m:\n",
    "                references.append(m[\"tool_calls\"])\n",
    "                break\n",
    "        else:\n",
    "            references.append([])\n",
    "        # Generate\n",
    "        text = tokenizer.apply_chat_template(msgs, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "        pred = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n",
    "        predictions.append(pred)\n",
    "    return predictions, references"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, device_map=\"auto\")\n",
    "base_preds, references = run_inference(base_model, tokenizer, test_samples, tools)\n",
    "base_metrics = compute_full_evaluation(base_preds, references, available_tools)\n",
    "print(\"Base model metrics:\")\n",
    "for k, v in base_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ft_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, device_map=\"auto\")\n",
    "ft_model = PeftModel.from_pretrained(ft_model, ADAPTER_PATH)\n",
    "ft_preds, _ = run_inference(ft_model, tokenizer, test_samples, tools)\n",
    "ft_metrics = compute_full_evaluation(ft_preds, references, available_tools)\n",
    "print(\"Fine-tuned model metrics:\")\n",
    "for k, v in ft_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "metrics = list(base_metrics.keys())\n",
    "base_vals = [base_metrics[m] for m in metrics]\n",
    "ft_vals = [ft_metrics[m] for m in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "bars1 = ax.bar([i - width/2 for i in x], base_vals, width, label=\"Base Model\", color=\"steelblue\")\n",
    "bars2 = ax.bar([i + width/2 for i in x], ft_vals, width, label=\"Fine-Tuned\", color=\"coral\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Base vs Fine-Tuned Model Metrics\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace(\"_\", \"\\n\") for m in metrics], fontsize=9)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "for bar in bars1 + bars2:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + 0.02, f\"{h:.2f}\", ha=\"center\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Sample-by-Sample Comparison ===\\n\")\n",
    "for i, (bp, fp, ref) in enumerate(zip(base_preds, ft_preds, references)):\n",
    "    base_tc = parse_tool_calls_from_output(bp)\n",
    "    ft_tc = parse_tool_calls_from_output(fp)\n",
    "    ref_names = [tc[\"function\"][\"name\"] for tc in ref] if ref else []\n",
    "    base_names = [tc.get(\"name\", tc.get(\"function\", {}).get(\"name\", \"?\")) for tc in base_tc]\n",
    "    ft_names = [tc.get(\"name\", tc.get(\"function\", {}).get(\"name\", \"?\")) for tc in ft_tc]\n",
    "    status = \"\\u2713\" if ft_names == ref_names else \"\\u2717\"\n",
    "    print(f\"Sample {i+1} {status}\")\n",
    "    print(f\"  Reference:  {ref_names}\")\n",
    "    print(f\"  Base:       {base_names}\")\n",
    "    print(f\"  Fine-tuned: {ft_names}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Compare improvements in tool name accuracy, parameter accuracy, and JSON validity\n",
    "- Lower hallucination rate is better\n",
    "- Check individual samples to understand error patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}